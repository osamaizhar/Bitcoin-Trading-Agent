{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Trading Agent - Data Collection\n",
    "\n",
    "This notebook handles all data collection from multiple sources:\n",
    "- Investing.com Bitcoin historical data (via Crawl4AI)\n",
    "- CoinMarketCap API for current prices\n",
    "- Yahoo Finance as backup\n",
    "- Binance API for additional market data\n",
    "\n",
    "## Observations Log\n",
    "We will document key findings and observations after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully\n",
      "Environment variables loaded: True\n",
      "Python paths added: ['..', '../src', 'd:\\\\Apziva\\\\Project 5\\\\Bitcoin-Trading-Agent\\\\env\\\\Scripts\\\\python313.zip']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fix Python path for imports - use absolute path\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "project_root = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'notebooks' else current_dir\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "\n",
    "# Add both possible paths to ensure imports work\n",
    "if os.path.exists(src_path):\n",
    "    sys.path.insert(0, src_path)\n",
    "    sys.path.insert(0, project_root)\n",
    "else:\n",
    "    # Fallback paths \n",
    "    sys.path.insert(0, '../src')\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"Environment variables loaded: {os.path.exists('../.env') or os.path.exists('.env')}\")\n",
    "print(f\"Python paths added: {[p for p in sys.path[:3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawl4AI Setup for Investing.com\n",
    "\n",
    "Using Crawl4AI to scrape Bitcoin historical data from Investing.com with advanced features like JavaScript rendering and anti-detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Using Enhanced DataCollector with Multiple Sources...\n",
      "‚ö†Ô∏è Direct import failed: No module named 'data_collector'\n",
      "‚ö†Ô∏è src module import failed: No module named 'src'\n",
      "‚ö†Ô∏è All import strategies failed:\n",
      "  - Direct: No module named 'data_collector'\n",
      "  - Module: No module named 'src'\n",
      "  - Path: No module named 'data_collector'\n",
      "‚ùå Enhanced DataCollector not available\n",
      "Falling back to original scraping methods...\n",
      "\n",
      "üîÑ Trying fallback scraping method...\n",
      "‚ùå Fallback scraping method also failed\n"
     ]
    }
   ],
   "source": [
    "from crawl4ai import AsyncWebCrawler\n",
    "from crawl4ai.extraction_strategy import LLMExtractionStrategy, LLMConfig\n",
    "import json\n",
    "\n",
    "async def scrape_investing_btc_data():\n",
    "    \"\"\"\n",
    "    Scrape Bitcoin historical data from Investing.com using Crawl4AI\n",
    "    \"\"\"\n",
    "    url = \"https://www.investing.com/crypto/bitcoin/historical-data\"\n",
    "\n",
    "    # Define extraction strategy using Groq with Llama 3.3 70B\n",
    "    extraction_strategy = LLMExtractionStrategy(\n",
    "        llm_config=LLMConfig(\n",
    "            provider=\"groq\",\n",
    "            api_token=os.getenv('GROQ_API_KEY'),\n",
    "            model=\"llama-3.3-70b-versatile\"\n",
    "        ),\n",
    "        instruction=\"\"\"\n",
    "        Extract the historical Bitcoin price data from the table. \n",
    "        Return a JSON array with objects containing:\n",
    "        - date: The date in YYYY-MM-DD format\n",
    "        - price: The closing price as a number\n",
    "        - open: The opening price as a number  \n",
    "        - high: The highest price as a number\n",
    "        - low: The lowest price as a number\n",
    "        - volume: The volume as a number\n",
    "        - change_pct: The percentage change as a number\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=url,\n",
    "            extraction_strategy=extraction_strategy,\n",
    "            css_selector=\".historical-data-table, table[data-test='historical-data-table']\",\n",
    "            wait_for=\"css:.historical-data-table\",\n",
    "            timeout=30000\n",
    "        )\n",
    "\n",
    "        return result\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# NEW: Try enhanced DataCollector with multiple sources first\n",
    "print(\"üîÑ Using Enhanced DataCollector with Multiple Sources...\")\n",
    "\n",
    "# Multiple import strategies to handle different path configurations\n",
    "collector = None\n",
    "enhanced_available = False\n",
    "\n",
    "# Strategy 1: Try direct import from data_collector\n",
    "try:\n",
    "    from data_collector import DataCollector\n",
    "    print(\"‚úÖ Imported DataCollector directly\")\n",
    "    enhanced_available = True\n",
    "except ImportError as e1:\n",
    "    print(f\"‚ö†Ô∏è Direct import failed: {e1}\")\n",
    "    \n",
    "    # Strategy 2: Try import from src.data_collector\n",
    "    try:\n",
    "        from src.data_collector import DataCollector\n",
    "        print(\"‚úÖ Imported DataCollector from src module\")\n",
    "        enhanced_available = True\n",
    "    except ImportError as e2:\n",
    "        print(f\"‚ö†Ô∏è src module import failed: {e2}\")\n",
    "        \n",
    "        # Strategy 3: Try with sys.path manipulation\n",
    "        try:\n",
    "            import sys\n",
    "            import os\n",
    "            project_root = os.path.dirname(os.path.abspath('.'))\n",
    "            src_path = os.path.join(project_root, 'src')\n",
    "            if src_path not in sys.path:\n",
    "                sys.path.insert(0, src_path)\n",
    "            \n",
    "            from data_collector import DataCollector\n",
    "            print(\"‚úÖ Imported DataCollector with path manipulation\")\n",
    "            enhanced_available = True\n",
    "        except ImportError as e3:\n",
    "            print(f\"‚ö†Ô∏è All import strategies failed:\")\n",
    "            print(f\"  - Direct: {e1}\")\n",
    "            print(f\"  - Module: {e2}\")  \n",
    "            print(f\"  - Path: {e3}\")\n",
    "            enhanced_available = False\n",
    "\n",
    "# If enhanced collector is available, try to use it\n",
    "if enhanced_available:\n",
    "    try:\n",
    "        # Initialize enhanced data collector\n",
    "        collector = DataCollector()\n",
    "        \n",
    "        print(f\"Available sources: {list(collector.sources.keys())}\")\n",
    "        \n",
    "        # Try collecting data from all sources\n",
    "        all_data = collector.collect_all_data(period='6mo')\n",
    "        \n",
    "        print(f\"\\nüìä Enhanced Data Collection Results:\")\n",
    "        successful_sources = []\n",
    "        for source, data in all_data.items():\n",
    "            if data is not None and not data.empty:\n",
    "                print(f\"‚úÖ {source}: {len(data)} records\")\n",
    "                # Save individual source data\n",
    "                data.to_csv(f'../data/btc_{source}_enhanced.csv', index=False)\n",
    "                successful_sources.append(source)\n",
    "            else:\n",
    "                print(f\"‚ùå {source}: No data\")\n",
    "        \n",
    "        # Get current price from best source\n",
    "        current_price = collector.get_current_price()\n",
    "        if current_price:\n",
    "            print(f\"\\nüí∞ Current BTC Price from enhanced collector: ${current_price:,.2f}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Could not get current price from enhanced collector\")\n",
    "            \n",
    "        # Combine successful sources\n",
    "        if successful_sources:\n",
    "            try:\n",
    "                # Try to import standardize_dataframes function\n",
    "                if enhanced_available:\n",
    "                    try:\n",
    "                        from data_collector import standardize_dataframes\n",
    "                    except ImportError:\n",
    "                        try:\n",
    "                            from src.data_collector import standardize_dataframes\n",
    "                        except ImportError:\n",
    "                            print(\"‚ö†Ô∏è Could not import standardize_dataframes, using local function\")\n",
    "                            # Define local version if import fails\n",
    "                            def standardize_dataframes(*dataframes, source_names=None):\n",
    "                                if source_names is None:\n",
    "                                    source_names = [f'source_{i}' for i in range(len(dataframes))]\n",
    "                                \n",
    "                                combined_data = []\n",
    "                                for df, source in zip(dataframes, source_names):\n",
    "                                    if df is not None and not df.empty:\n",
    "                                        df_copy = df.copy()\n",
    "                                        df_copy['source'] = source\n",
    "                                        combined_data.append(df_copy)\n",
    "                                \n",
    "                                if combined_data:\n",
    "                                    return pd.concat(combined_data, ignore_index=True)\n",
    "                                return None\n",
    "                \n",
    "                successful_dfs = [all_data[source] for source in successful_sources]\n",
    "                enhanced_combined = standardize_dataframes(*successful_dfs, source_names=successful_sources)\n",
    "                \n",
    "                if enhanced_combined is not None:\n",
    "                    print(f\"\\n‚úÖ Enhanced Combined Dataset: {len(enhanced_combined)} records\")\n",
    "                    print(f\"Sources: {enhanced_combined['source'].unique()}\")\n",
    "                    enhanced_combined.to_csv('../data/btc_enhanced_combined.csv', index=False)\n",
    "                    \n",
    "                    # Set this as our primary combined data\n",
    "                    combined_btc_data_enhanced = enhanced_combined\n",
    "            except Exception as combine_error:\n",
    "                print(f\"‚ö†Ô∏è Error combining enhanced data: {combine_error}\")\n",
    "                \n",
    "    except Exception as collector_error:\n",
    "        print(f\"‚ùå Enhanced DataCollector runtime error: {collector_error}\")\n",
    "        print(\"Falling back to original scraping methods...\")\n",
    "        enhanced_available = False\n",
    "else:\n",
    "    print(\"‚ùå Enhanced DataCollector not available\")\n",
    "    print(\"Falling back to original scraping methods...\")\n",
    "\n",
    "# FALLBACK: Original scraping methods if enhanced collector fails\n",
    "if not enhanced_available or 'successful_sources' not in locals() or not successful_sources:\n",
    "    def fallback_investing_scraper():\n",
    "        \"\"\"\n",
    "        Fallback method using requests + BeautifulSoup if all else fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from bs4 import BeautifulSoup\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            url = \"https://www.investing.com/crypto/bitcoin/historical-data\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find the historical data table\n",
    "                table = soup.find('table', {'data-test': 'historical-data-table'})\n",
    "                if not table:\n",
    "                    table = soup.find('table', class_='historical-data-table')\n",
    "                \n",
    "                if table:\n",
    "                    rows = table.find_all('tr')[1:]  # Skip header\n",
    "                    \n",
    "                    data = []\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all('td')\n",
    "                        if len(cols) >= 6:\n",
    "                            date_str = cols[0].text.strip()\n",
    "                            price = cols[1].text.replace(',', '').replace('$', '').strip()\n",
    "                            open_price = cols[2].text.replace(',', '').replace('$', '').strip()\n",
    "                            high = cols[3].text.replace(',', '').replace('$', '').strip()\n",
    "                            low = cols[4].text.replace(',', '').replace('$', '').strip()\n",
    "                            volume = cols[5].text.replace(',', '').strip()\n",
    "                            \n",
    "                            data.append({\n",
    "                                'date': date_str,\n",
    "                                'price': float(price) if price else None,\n",
    "                                'open': float(open_price) if open_price else None,\n",
    "                                'high': float(high) if high else None,\n",
    "                                'low': float(low) if low else None,\n",
    "                                'volume': volume\n",
    "                            })\n",
    "                    \n",
    "                    return pd.DataFrame(data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Fallback scraper error: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "    print(\"\\nüîÑ Trying fallback scraping method...\")\n",
    "    investing_df = fallback_investing_scraper()\n",
    "    \n",
    "    if investing_df is not None:\n",
    "        print(f\"‚úÖ Fallback method successful: {len(investing_df)} records\")\n",
    "        investing_df.to_csv('../data/btc_investing_raw.csv', index=False)\n",
    "    else:\n",
    "        print(\"‚ùå Fallback scraping method also failed\")\n",
    "else:\n",
    "    if 'successful_sources' in locals() and successful_sources:\n",
    "        print(f\"\\n‚úÖ Enhanced data collection successful from {len(successful_sources)} sources\")\n",
    "        print(\"Skipping fallback scraper\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Enhanced collector loaded but no successful sources found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 5 Scraper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# url = \"https://www.investing.com/crypto/bitcoin/historical-data\"\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0\",\n",
    "# }\n",
    "\n",
    "# # Fetch the page\n",
    "# resp = requests.get(url, headers=headers)\n",
    "\n",
    "# # Extract first table (raw form)\n",
    "# raw_df = pd.read_html(resp.text)[0]\n",
    "\n",
    "# print(raw_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 1: Investing.com Scraping Results\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Document success rate of Crawl4AI scraping\n",
    "- [ ] Note data quality and completeness\n",
    "- [ ] Record any anti-bot detection issues\n",
    "- [ ] Validate date ranges and data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CoinMarketCap API Integration\n",
    "\n",
    "Getting current Bitcoin price and market data from CoinMarketCap API for real-time updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Current Bitcoin data from CoinMarketCap:\n",
      "Price: $117,494.00\n",
      "24h Change: -1.35%\n",
      "Volume 24h: $63,639,141,349\n",
      "Market Cap: $2,338,964,050,347\n"
     ]
    }
   ],
   "source": [
    "def get_coinmarketcap_data():\n",
    "    \"\"\"\n",
    "    Fetch current Bitcoin data from CoinMarketCap API\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('COINMARKETCAP_API_KEY')\n",
    "    \n",
    "    if not api_key or api_key == 'your_coinmarketcap_api_key_here':\n",
    "        print(\"‚ö†Ô∏è CoinMarketCap API key not configured\")\n",
    "        return None\n",
    "    \n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n",
    "    parameters = {\n",
    "        'symbol': 'BTC',\n",
    "        'convert': 'USD'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accepts': 'application/json',\n",
    "        'X-CMC_PRO_API_KEY': api_key,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=parameters)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200 and 'data' in data:\n",
    "            btc_data = data['data']['BTC']\n",
    "            quote = btc_data['quote']['USD']\n",
    "            \n",
    "            current_data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'price': quote['price'],\n",
    "                'volume_24h': quote['volume_24h'],\n",
    "                'percent_change_1h': quote['percent_change_1h'],\n",
    "                'percent_change_24h': quote['percent_change_24h'],\n",
    "                'percent_change_7d': quote['percent_change_7d'],\n",
    "                'market_cap': quote['market_cap'],\n",
    "                'last_updated': quote['last_updated']\n",
    "            }\n",
    "            \n",
    "            return current_data\n",
    "        else:\n",
    "            print(f\"‚ùå CoinMarketCap API error: {data}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching CoinMarketCap data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fetch current data\n",
    "cmc_data = get_coinmarketcap_data()\n",
    "\n",
    "if cmc_data:\n",
    "    print(\"‚úÖ Current Bitcoin data from CoinMarketCap:\")\n",
    "    print(f\"Price: ${cmc_data['price']:,.2f}\")\n",
    "    print(f\"24h Change: {cmc_data['percent_change_24h']:.2f}%\")\n",
    "    print(f\"Volume 24h: ${cmc_data['volume_24h']:,.0f}\")\n",
    "    print(f\"Market Cap: ${cmc_data['market_cap']:,.0f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CoinMarketCap data not available - using backup sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 2: CoinMarketCap API Performance\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Note API response time and reliability\n",
    "- [ ] Document rate limits encountered\n",
    "- [ ] Compare price accuracy with other sources\n",
    "- [ ] Record data freshness (last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Yahoo Finance Integration\n",
    "\n",
    "Using yfinance as a reliable backup source for Bitcoin historical and current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yahoo Finance data: 182 records\n",
      "Date range: 2025-02-16 to 2025-08-16\n",
      "Current price: $117,494.00\n",
      "Latest close: $117,494.00\n",
      "\n",
      "Recent data from Yahoo Finance:\n",
      "           date           open           high            low          close  \\\n",
      "177  2025-08-12  118717.664062  120302.468750  118228.718750  120172.906250   \n",
      "178  2025-08-13  120168.976562  123682.453125  118939.632812  123344.062500   \n",
      "179  2025-08-14  123339.398438  124457.117188  117254.882812  118359.578125   \n",
      "180  2025-08-15  118365.781250  119332.312500  116864.570312  117398.351562   \n",
      "181  2025-08-16  117411.218750  117948.859375  117355.820312  117494.000000   \n",
      "\n",
      "           volume  \n",
      "177   72803657984  \n",
      "178   90904808795  \n",
      "179  104055627395  \n",
      "180   68665353159  \n",
      "181   63639142400  \n"
     ]
    }
   ],
   "source": [
    "def get_yahoo_btc_data(period='1y'):\n",
    "    \"\"\"\n",
    "    Fetch Bitcoin data from Yahoo Finance\n",
    "    period options: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btc_ticker = yf.Ticker(\"BTC-USD\")\n",
    "        hist_data = btc_ticker.history(period=period)\n",
    "        \n",
    "        if not hist_data.empty:\n",
    "            # Reset index to get date as column\n",
    "            hist_data = hist_data.reset_index()\n",
    "            \n",
    "            # Rename columns to match our standard format\n",
    "            hist_data.columns = [col.lower() for col in hist_data.columns]\n",
    "            hist_data['date'] = hist_data['date'].dt.strftime('%Y-%m-%d')\n",
    "            hist_data['price'] = hist_data['close']\n",
    "            \n",
    "            # Get current info\n",
    "            info = btc_ticker.info\n",
    "            current_price = info.get('regularMarketPrice', hist_data['close'].iloc[-1])\n",
    "            \n",
    "            print(f\"‚úÖ Yahoo Finance data: {len(hist_data)} records\")\n",
    "            print(f\"Date range: {hist_data['date'].iloc[0]} to {hist_data['date'].iloc[-1]}\")\n",
    "            print(f\"Current price: ${current_price:,.2f}\")\n",
    "            print(f\"Latest close: ${hist_data['close'].iloc[-1]:,.2f}\")\n",
    "            \n",
    "            return hist_data, current_price\n",
    "        else:\n",
    "            print(\"‚ùå No data returned from Yahoo Finance\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching Yahoo Finance data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Fetch Yahoo Finance data\n",
    "yahoo_df, yahoo_current = get_yahoo_btc_data(period='6mo')\n",
    "\n",
    "if yahoo_df is not None:\n",
    "    # Save Yahoo data\n",
    "    yahoo_df.to_csv('../data/btc_yahoo_raw.csv', index=False)\n",
    "    \n",
    "    # Show recent data\n",
    "    print(\"\\nRecent data from Yahoo Finance:\")\n",
    "    print(yahoo_df[['date', 'open', 'high', 'low', 'close', 'volume']].tail())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Yahoo Finance data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 3: Yahoo Finance Data Quality\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Compare data consistency with other sources\n",
    "- [ ] Note any gaps or anomalies in historical data\n",
    "- [ ] Document volume data accuracy\n",
    "- [ ] Record API response reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Integration and Standardization\n",
    "\n",
    "Combining data from all sources into a standardized format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 182 records from yahoo\n",
      "\n",
      "‚úÖ Combined dataset: 182 total records\n",
      "Date range: 2025-02-16 00:00:00 to 2025-08-16 00:00:00\n",
      "Sources: ['yahoo']\n",
      "\n",
      "Data summary by source:\n",
      "        date                           close                     \n",
      "       count        min        max      mean       min        max\n",
      "source                                                           \n",
      "yahoo    182 2025-02-16 2025-08-16  100382.4  76271.95  123344.06\n"
     ]
    }
   ],
   "source": [
    "def standardize_btc_data(*dataframes, source_names=None):\n",
    "    \"\"\"\n",
    "    Standardize and combine Bitcoin data from multiple sources\n",
    "    \"\"\"\n",
    "    if source_names is None:\n",
    "        source_names = [f'source_{i}' for i in range(len(dataframes))]\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for df, source in zip(dataframes, source_names):\n",
    "        if df is not None and not df.empty:\n",
    "            # Create a copy to avoid modifying original\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Ensure we have required columns\n",
    "            required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "            \n",
    "            # Map common column variations\n",
    "            if 'price' in df_copy.columns and 'close' not in df_copy.columns:\n",
    "                df_copy['close'] = df_copy['price']\n",
    "            \n",
    "            # Convert date to datetime if it's not already\n",
    "            if 'date' in df_copy.columns:\n",
    "                df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "            \n",
    "            # Add source column\n",
    "            df_copy['source'] = source\n",
    "            \n",
    "            # Select and reorder columns\n",
    "            available_cols = [col for col in required_cols if col in df_copy.columns]\n",
    "            df_final = df_copy[available_cols + ['source']].copy()\n",
    "            \n",
    "            combined_data.append(df_final)\n",
    "            print(f\"‚úÖ Processed {len(df_final)} records from {source}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No data available from {source}\")\n",
    "    \n",
    "    if combined_data:\n",
    "        # Combine all dataframes\n",
    "        final_df = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on date and source\n",
    "        final_df = final_df.drop_duplicates(subset=['date', 'source'])\n",
    "        \n",
    "        # Sort by date\n",
    "        final_df = final_df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Combine available data sources\n",
    "available_dfs = []\n",
    "source_names = []\n",
    "\n",
    "if 'investing_df' in locals() and investing_df is not None:\n",
    "    available_dfs.append(investing_df)\n",
    "    source_names.append('investing')\n",
    "\n",
    "if 'yahoo_df' in locals() and yahoo_df is not None:\n",
    "    available_dfs.append(yahoo_df)\n",
    "    source_names.append('yahoo')\n",
    "\n",
    "# Standardize and combine data\n",
    "if available_dfs:\n",
    "    combined_btc_data = standardize_btc_data(*available_dfs, source_names=source_names)\n",
    "    \n",
    "    if combined_btc_data is not None:\n",
    "        print(f\"\\n‚úÖ Combined dataset: {len(combined_btc_data)} total records\")\n",
    "        print(f\"Date range: {combined_btc_data['date'].min()} to {combined_btc_data['date'].max()}\")\n",
    "        print(f\"Sources: {combined_btc_data['source'].unique()}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        combined_btc_data.to_csv('../data/btc_combined_raw.csv', index=False)\n",
    "        \n",
    "        # Show data summary\n",
    "        print(\"\\nData summary by source:\")\n",
    "        print(combined_btc_data.groupby('source').agg({\n",
    "            'date': ['count', 'min', 'max'],\n",
    "            'close': ['mean', 'min', 'max']\n",
    "        }).round(2))\n",
    "    else:\n",
    "        print(\"‚ùå Failed to combine data sources\")\n",
    "else:\n",
    "    print(\"‚ùå No data sources available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 4: Data Integration Results\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Document data overlap and gaps between sources\n",
    "- [ ] Note price discrepancies between sources\n",
    "- [ ] Record data quality metrics\n",
    "- [ ] Identify preferred data source for different time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks\n",
    "\n",
    "Performing comprehensive data validation and quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Data Quality Assessment\n",
      "==================================================\n",
      "üìä Dataset Shape: (182, 7)\n",
      "üìÖ Date Range: 2025-02-16 00:00:00 to 2025-08-16 00:00:00\n",
      "üî¢ Total Days: 181 days\n",
      "\n",
      "üîç Missing Values:\n",
      "  date: ‚úÖ No missing values\n",
      "  open: ‚úÖ No missing values\n",
      "  high: ‚úÖ No missing values\n",
      "  low: ‚úÖ No missing values\n",
      "  close: ‚úÖ No missing values\n",
      "  volume: ‚úÖ No missing values\n",
      "  source: ‚úÖ No missing values\n",
      "\n",
      "üìã Data Types:\n",
      "  date: datetime64[ns]\n",
      "  open: float64\n",
      "  high: float64\n",
      "  low: float64\n",
      "  close: float64\n",
      "  volume: int64\n",
      "  source: object\n",
      "\n",
      "üí∞ Price Statistics:\n",
      "  count: $182.00\n",
      "  mean: $100,382.40\n",
      "  std: $12,759.50\n",
      "  min: $76,271.95\n",
      "  25%: $86,865.89\n",
      "  50%: $103,642.03\n",
      "  75%: $109,182.90\n",
      "  max: $123,344.06\n",
      "\n",
      "‚ö†Ô∏è Price Anomaly Checks:\n",
      "  Negative/Zero prices: 0\n",
      "  Extreme daily changes (>50%): 0\n",
      "\n",
      "üîÑ Duplicate Checks:\n",
      "  Duplicate dates: 0\n",
      "\n",
      "üìà Data Source Distribution:\n",
      "  yahoo: 182 records (100.0%)\n"
     ]
    }
   ],
   "source": [
    "def perform_data_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No data to check\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üîç Data Quality Assessment\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "    print(f\"üìÖ Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"üî¢ Total Days: {(df['date'].max() - df['date'].min()).days} days\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nüîç Missing Values:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    for col, missing in missing_summary.items():\n",
    "        if missing > 0:\n",
    "            pct = (missing / len(df)) * 100\n",
    "            print(f\"  {col}: {missing} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {col}: ‚úÖ No missing values\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    # Price statistics\n",
    "    if 'close' in df.columns:\n",
    "        print(\"\\nüí∞ Price Statistics:\")\n",
    "        price_stats = df['close'].describe()\n",
    "        for stat, value in price_stats.items():\n",
    "            print(f\"  {stat}: ${value:,.2f}\")\n",
    "        \n",
    "        # Price anomalies\n",
    "        print(\"\\n‚ö†Ô∏è Price Anomaly Checks:\")\n",
    "        \n",
    "        # Check for negative prices\n",
    "        negative_prices = (df['close'] <= 0).sum()\n",
    "        print(f\"  Negative/Zero prices: {negative_prices}\")\n",
    "        \n",
    "        # Check for extreme price changes (>50% in one day)\n",
    "        df_sorted = df.sort_values('date')\n",
    "        price_changes = df_sorted['close'].pct_change().abs()\n",
    "        extreme_changes = (price_changes > 0.5).sum()\n",
    "        print(f\"  Extreme daily changes (>50%): {extreme_changes}\")\n",
    "        \n",
    "        if extreme_changes > 0:\n",
    "            extreme_dates = df_sorted[price_changes > 0.5]['date'].tolist()\n",
    "            print(f\"    Dates with extreme changes: {extreme_dates[:5]}\")\n",
    "    \n",
    "    # Duplicate checks\n",
    "    print(\"\\nüîÑ Duplicate Checks:\")\n",
    "    date_duplicates = df['date'].duplicated().sum()\n",
    "    print(f\"  Duplicate dates: {date_duplicates}\")\n",
    "    \n",
    "    # Source distribution\n",
    "    if 'source' in df.columns:\n",
    "        print(\"\\nüìà Data Source Distribution:\")\n",
    "        source_counts = df['source'].value_counts()\n",
    "        for source, count in source_counts.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {source}: {count} records ({pct:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': missing_summary.to_dict(),\n",
    "        'date_range': (df['date'].min(), df['date'].max()),\n",
    "        'price_stats': price_stats.to_dict() if 'close' in df.columns else None,\n",
    "        'anomalies': {\n",
    "            'negative_prices': negative_prices if 'close' in df.columns else 0,\n",
    "            'extreme_changes': extreme_changes if 'close' in df.columns else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run quality checks on combined data\n",
    "if 'combined_btc_data' in locals() and combined_btc_data is not None:\n",
    "    quality_report = perform_data_quality_checks(combined_btc_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No combined data available for quality checks\")\n",
    "    quality_report = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 5: Data Quality Assessment\n",
    "\n",
    "**Critical Findings:**\n",
    "- [ ] Document completeness percentage by source\n",
    "- [ ] Note any data quality issues requiring cleaning\n",
    "- [ ] Record price anomalies and potential causes\n",
    "- [ ] Assess suitability for trading strategy development\n",
    "\n",
    "**Next Steps:**\n",
    "- [ ] Address missing data through interpolation or source switching\n",
    "- [ ] Implement data cleaning pipeline for anomalies\n",
    "- [ ] Set up automated data quality monitoring\n",
    "- [ ] Prepare data for EDA in next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Summary\n",
    "\n",
    "Final data export and summary for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Data Collection Summary\n",
      "========================================\n",
      "‚úÖ Successful sources: 2/3\n",
      "üìä Total records collected: 182\n",
      "üìÅ Files created: 2\n",
      "üèÅ Ready for EDA: ‚úÖ\n",
      "\n",
      "üìÖ Date range: 2025-02-16T00:00:00 to 2025-08-16T00:00:00\n",
      "\n",
      "üéØ Next step: Run notebook 02_eda_analysis.ipynb for exploratory data analysis\n"
     ]
    }
   ],
   "source": [
    "# Create summary of all data collection efforts\n",
    "collection_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'sources_attempted': ['investing.com', 'coinmarketcap', 'yahoo_finance'],\n",
    "    'sources_successful': [],\n",
    "    'total_records': 0,\n",
    "    'date_range': None,\n",
    "    'files_created': [],\n",
    "    'quality_report': quality_report\n",
    "}\n",
    "\n",
    "# Check which sources were successful\n",
    "if 'investing_df' in locals() and investing_df is not None:\n",
    "    collection_summary['sources_successful'].append('investing.com')\n",
    "    collection_summary['files_created'].append('btc_investing_raw.csv')\n",
    "\n",
    "if 'yahoo_df' in locals() and yahoo_df is not None:\n",
    "    collection_summary['sources_successful'].append('yahoo_finance')\n",
    "    collection_summary['files_created'].append('btc_yahoo_raw.csv')\n",
    "\n",
    "if 'cmc_data' in locals() and cmc_data is not None:\n",
    "    collection_summary['sources_successful'].append('coinmarketcap')\n",
    "\n",
    "if 'combined_btc_data' in locals() and combined_btc_data is not None:\n",
    "    collection_summary['total_records'] = len(combined_btc_data)\n",
    "    collection_summary['date_range'] = [\n",
    "        combined_btc_data['date'].min().isoformat(),\n",
    "        combined_btc_data['date'].max().isoformat()\n",
    "    ]\n",
    "    collection_summary['files_created'].append('btc_combined_raw.csv')\n",
    "\n",
    "# Save collection summary\n",
    "import json\n",
    "with open('../data/collection_summary.json', 'w') as f:\n",
    "    json.dump(collection_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"üìã Data Collection Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"‚úÖ Successful sources: {len(collection_summary['sources_successful'])}/{len(collection_summary['sources_attempted'])}\")\n",
    "print(f\"üìä Total records collected: {collection_summary['total_records']}\")\n",
    "print(f\"üìÅ Files created: {len(collection_summary['files_created'])}\")\n",
    "print(f\"üèÅ Ready for EDA: {'‚úÖ' if collection_summary['total_records'] > 0 else '‚ùå'}\")\n",
    "\n",
    "if collection_summary['total_records'] > 0:\n",
    "    print(f\"\\nüìÖ Date range: {collection_summary['date_range'][0]} to {collection_summary['date_range'][1]}\")\n",
    "    print(\"\\nüéØ Next step: Run notebook 02_eda_analysis.ipynb for exploratory data analysis\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data collected - check API keys and network connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Observation: Data Collection Complete\n",
    "\n",
    "**Summary of Results:**\n",
    "- [ ] Record final success rate of all data sources\n",
    "- [ ] Document total data points collected\n",
    "- [ ] Note any persistent issues with specific sources\n",
    "- [ ] Confirm readiness for EDA phase\n",
    "\n",
    "**Key Takeaways for Trading Strategy:**\n",
    "1. Data reliability ranking by source\n",
    "2. Recommended fallback strategy for data outages\n",
    "3. Data freshness considerations for live trading\n",
    "4. Quality thresholds for strategy execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
