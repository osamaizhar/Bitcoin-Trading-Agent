{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Trading Agent - Data Collection\n",
    "\n",
    "This notebook handles all data collection from multiple sources:\n",
    "- Investing.com Bitcoin historical data (via Crawl4AI)\n",
    "- CoinMarketCap API for current prices\n",
    "- Yahoo Finance as backup\n",
    "- Binance API for additional market data\n",
    "\n",
    "## Observations Log\n",
    "We will document key findings and observations after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"Environment variables loaded: {os.path.exists('../.env')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawl4AI Setup for Investing.com\n",
    "\n",
    "Using Crawl4AI to scrape Bitcoin historical data from Investing.com with advanced features like JavaScript rendering and anti-detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nimport json\n\nasync def scrape_investing_btc_data():\n    \"\"\"\n    Scrape Bitcoin historical data from Investing.com using Crawl4AI\n    \"\"\"\n    url = \"https://www.investing.com/crypto/bitcoin/historical-data\"\n    \n    # Define extraction strategy using Groq with Llama 3.3 70B\n    extraction_strategy = LLMExtractionStrategy(\n        provider=\"groq\",\n        api_token=os.getenv('GROQ_API_KEY'),\n        model=\"llama-3.3-70b-versatile\",\n        instruction=\"\"\"\n        Extract the historical Bitcoin price data from the table. \n        Return a JSON array with objects containing:\n        - date: The date in YYYY-MM-DD format\n        - price: The closing price as a number\n        - open: The opening price as a number  \n        - high: The highest price as a number\n        - low: The lowest price as a number\n        - volume: The volume as a number\n        - change_pct: The percentage change as a number\n        \"\"\"\n    )\n    \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=extraction_strategy,\n            css_selector=\".historical-data-table, table[data-test='historical-data-table']\",\n            wait_for=\"css:.historical-data-table\",\n            timeout=30000\n        )\n        \n        return result\n\n# Run the scraper\ntry:\n    result = await scrape_investing_btc_data()\n    \n    if result.extracted_content:\n        # Parse the JSON response\n        btc_data = json.loads(result.extracted_content)\n        investing_df = pd.DataFrame(btc_data)\n        \n        print(f\"‚úÖ Successfully scraped {len(investing_df)} records from Investing.com\")\n        print(f\"Date range: {investing_df['date'].min()} to {investing_df['date'].max()}\")\n        print(investing_df.head())\n        \n        # Save to CSV\n        investing_df.to_csv('../data/btc_investing_raw.csv', index=False)\n        \n    else:\n        print(\"‚ùå No data extracted from Investing.com\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error scraping Investing.com: {str(e)}\")\n    investing_df = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 1: Investing.com Scraping Results\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Document success rate of Crawl4AI scraping\n",
    "- [ ] Note data quality and completeness\n",
    "- [ ] Record any anti-bot detection issues\n",
    "- [ ] Validate date ranges and data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# If scraping failed, create a fallback function\n",
    "def fallback_investing_scraper():\n",
    "    \"\"\"\n",
    "    Fallback method using requests + BeautifulSoup if Crawl4AI fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        url = \"https://www.investing.com/crypto/bitcoin/historical-data\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the historical data table\n",
    "            table = soup.find('table', {'data-test': 'historical-data-table'})\n",
    "            if not table:\n",
    "                table = soup.find('table', class_='historical-data-table')\n",
    "            \n",
    "            if table:\n",
    "                rows = table.find_all('tr')[1:]  # Skip header\n",
    "                \n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) >= 6:\n",
    "                        date_str = cols[0].text.strip()\n",
    "                        price = cols[1].text.replace(',', '').replace('$', '').strip()\n",
    "                        open_price = cols[2].text.replace(',', '').replace('$', '').strip()\n",
    "                        high = cols[3].text.replace(',', '').replace('$', '').strip()\n",
    "                        low = cols[4].text.replace(',', '').replace('$', '').strip()\n",
    "                        volume = cols[5].text.replace(',', '').strip()\n",
    "                        \n",
    "                        data.append({\n",
    "                            'date': date_str,\n",
    "                            'price': float(price) if price else None,\n",
    "                            'open': float(open_price) if open_price else None,\n",
    "                            'high': float(high) if high else None,\n",
    "                            'low': float(low) if low else None,\n",
    "                            'volume': volume\n",
    "                        })\n",
    "                \n",
    "                return pd.DataFrame(data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Fallback scraper error: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Try fallback if primary method failed\n",
    "if 'investing_df' not in locals() or investing_df is None:\n",
    "    print(\"Trying fallback scraping method...\")\n",
    "    investing_df = fallback_investing_scraper()\n",
    "    \n",
    "    if investing_df is not None:\n",
    "        print(f\"‚úÖ Fallback method successful: {len(investing_df)} records\")\n",
    "        investing_df.to_csv('../data/btc_investing_raw.csv', index=False)\n",
    "    else:\n",
    "        print(\"‚ùå Both scraping methods failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CoinMarketCap API Integration\n",
    "\n",
    "Getting current Bitcoin price and market data from CoinMarketCap API for real-time updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coinmarketcap_data():\n",
    "    \"\"\"\n",
    "    Fetch current Bitcoin data from CoinMarketCap API\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('COINMARKETCAP_API_KEY')\n",
    "    \n",
    "    if not api_key or api_key == 'your_coinmarketcap_api_key_here':\n",
    "        print(\"‚ö†Ô∏è CoinMarketCap API key not configured\")\n",
    "        return None\n",
    "    \n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n",
    "    parameters = {\n",
    "        'symbol': 'BTC',\n",
    "        'convert': 'USD'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accepts': 'application/json',\n",
    "        'X-CMC_PRO_API_KEY': api_key,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=parameters)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200 and 'data' in data:\n",
    "            btc_data = data['data']['BTC']\n",
    "            quote = btc_data['quote']['USD']\n",
    "            \n",
    "            current_data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'price': quote['price'],\n",
    "                'volume_24h': quote['volume_24h'],\n",
    "                'percent_change_1h': quote['percent_change_1h'],\n",
    "                'percent_change_24h': quote['percent_change_24h'],\n",
    "                'percent_change_7d': quote['percent_change_7d'],\n",
    "                'market_cap': quote['market_cap'],\n",
    "                'last_updated': quote['last_updated']\n",
    "            }\n",
    "            \n",
    "            return current_data\n",
    "        else:\n",
    "            print(f\"‚ùå CoinMarketCap API error: {data}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching CoinMarketCap data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fetch current data\n",
    "cmc_data = get_coinmarketcap_data()\n",
    "\n",
    "if cmc_data:\n",
    "    print(\"‚úÖ Current Bitcoin data from CoinMarketCap:\")\n",
    "    print(f\"Price: ${cmc_data['price']:,.2f}\")\n",
    "    print(f\"24h Change: {cmc_data['percent_change_24h']:.2f}%\")\n",
    "    print(f\"Volume 24h: ${cmc_data['volume_24h']:,.0f}\")\n",
    "    print(f\"Market Cap: ${cmc_data['market_cap']:,.0f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CoinMarketCap data not available - using backup sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 2: CoinMarketCap API Performance\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Note API response time and reliability\n",
    "- [ ] Document rate limits encountered\n",
    "- [ ] Compare price accuracy with other sources\n",
    "- [ ] Record data freshness (last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Yahoo Finance Integration\n",
    "\n",
    "Using yfinance as a reliable backup source for Bitcoin historical and current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_btc_data(period='1y'):\n",
    "    \"\"\"\n",
    "    Fetch Bitcoin data from Yahoo Finance\n",
    "    period options: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btc_ticker = yf.Ticker(\"BTC-USD\")\n",
    "        hist_data = btc_ticker.history(period=period)\n",
    "        \n",
    "        if not hist_data.empty:\n",
    "            # Reset index to get date as column\n",
    "            hist_data = hist_data.reset_index()\n",
    "            \n",
    "            # Rename columns to match our standard format\n",
    "            hist_data.columns = [col.lower() for col in hist_data.columns]\n",
    "            hist_data['date'] = hist_data['date'].dt.strftime('%Y-%m-%d')\n",
    "            hist_data['price'] = hist_data['close']\n",
    "            \n",
    "            # Get current info\n",
    "            info = btc_ticker.info\n",
    "            current_price = info.get('regularMarketPrice', hist_data['close'].iloc[-1])\n",
    "            \n",
    "            print(f\"‚úÖ Yahoo Finance data: {len(hist_data)} records\")\n",
    "            print(f\"Date range: {hist_data['date'].iloc[0]} to {hist_data['date'].iloc[-1]}\")\n",
    "            print(f\"Current price: ${current_price:,.2f}\")\n",
    "            print(f\"Latest close: ${hist_data['close'].iloc[-1]:,.2f}\")\n",
    "            \n",
    "            return hist_data, current_price\n",
    "        else:\n",
    "            print(\"‚ùå No data returned from Yahoo Finance\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching Yahoo Finance data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Fetch Yahoo Finance data\n",
    "yahoo_df, yahoo_current = get_yahoo_btc_data(period='6mo')\n",
    "\n",
    "if yahoo_df is not None:\n",
    "    # Save Yahoo data\n",
    "    yahoo_df.to_csv('../data/btc_yahoo_raw.csv', index=False)\n",
    "    \n",
    "    # Show recent data\n",
    "    print(\"\\nRecent data from Yahoo Finance:\")\n",
    "    print(yahoo_df[['date', 'open', 'high', 'low', 'close', 'volume']].tail())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Yahoo Finance data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 3: Yahoo Finance Data Quality\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Compare data consistency with other sources\n",
    "- [ ] Note any gaps or anomalies in historical data\n",
    "- [ ] Document volume data accuracy\n",
    "- [ ] Record API response reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Integration and Standardization\n",
    "\n",
    "Combining data from all sources into a standardized format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_btc_data(*dataframes, source_names=None):\n",
    "    \"\"\"\n",
    "    Standardize and combine Bitcoin data from multiple sources\n",
    "    \"\"\"\n",
    "    if source_names is None:\n",
    "        source_names = [f'source_{i}' for i in range(len(dataframes))]\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for df, source in zip(dataframes, source_names):\n",
    "        if df is not None and not df.empty:\n",
    "            # Create a copy to avoid modifying original\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Ensure we have required columns\n",
    "            required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "            \n",
    "            # Map common column variations\n",
    "            if 'price' in df_copy.columns and 'close' not in df_copy.columns:\n",
    "                df_copy['close'] = df_copy['price']\n",
    "            \n",
    "            # Convert date to datetime if it's not already\n",
    "            if 'date' in df_copy.columns:\n",
    "                df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "            \n",
    "            # Add source column\n",
    "            df_copy['source'] = source\n",
    "            \n",
    "            # Select and reorder columns\n",
    "            available_cols = [col for col in required_cols if col in df_copy.columns]\n",
    "            df_final = df_copy[available_cols + ['source']].copy()\n",
    "            \n",
    "            combined_data.append(df_final)\n",
    "            print(f\"‚úÖ Processed {len(df_final)} records from {source}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No data available from {source}\")\n",
    "    \n",
    "    if combined_data:\n",
    "        # Combine all dataframes\n",
    "        final_df = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on date and source\n",
    "        final_df = final_df.drop_duplicates(subset=['date', 'source'])\n",
    "        \n",
    "        # Sort by date\n",
    "        final_df = final_df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Combine available data sources\n",
    "available_dfs = []\n",
    "source_names = []\n",
    "\n",
    "if 'investing_df' in locals() and investing_df is not None:\n",
    "    available_dfs.append(investing_df)\n",
    "    source_names.append('investing')\n",
    "\n",
    "if 'yahoo_df' in locals() and yahoo_df is not None:\n",
    "    available_dfs.append(yahoo_df)\n",
    "    source_names.append('yahoo')\n",
    "\n",
    "# Standardize and combine data\n",
    "if available_dfs:\n",
    "    combined_btc_data = standardize_btc_data(*available_dfs, source_names=source_names)\n",
    "    \n",
    "    if combined_btc_data is not None:\n",
    "        print(f\"\\n‚úÖ Combined dataset: {len(combined_btc_data)} total records\")\n",
    "        print(f\"Date range: {combined_btc_data['date'].min()} to {combined_btc_data['date'].max()}\")\n",
    "        print(f\"Sources: {combined_btc_data['source'].unique()}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        combined_btc_data.to_csv('../data/btc_combined_raw.csv', index=False)\n",
    "        \n",
    "        # Show data summary\n",
    "        print(\"\\nData summary by source:\")\n",
    "        print(combined_btc_data.groupby('source').agg({\n",
    "            'date': ['count', 'min', 'max'],\n",
    "            'close': ['mean', 'min', 'max']\n",
    "        }).round(2))\n",
    "    else:\n",
    "        print(\"‚ùå Failed to combine data sources\")\n",
    "else:\n",
    "    print(\"‚ùå No data sources available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 4: Data Integration Results\n",
    "\n",
    "**Key Findings:**\n",
    "- [ ] Document data overlap and gaps between sources\n",
    "- [ ] Note price discrepancies between sources\n",
    "- [ ] Record data quality metrics\n",
    "- [ ] Identify preferred data source for different time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks\n",
    "\n",
    "Performing comprehensive data validation and quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No data to check\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üîç Data Quality Assessment\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "    print(f\"üìÖ Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"üî¢ Total Days: {(df['date'].max() - df['date'].min()).days} days\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nüîç Missing Values:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    for col, missing in missing_summary.items():\n",
    "        if missing > 0:\n",
    "            pct = (missing / len(df)) * 100\n",
    "            print(f\"  {col}: {missing} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {col}: ‚úÖ No missing values\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    # Price statistics\n",
    "    if 'close' in df.columns:\n",
    "        print(\"\\nüí∞ Price Statistics:\")\n",
    "        price_stats = df['close'].describe()\n",
    "        for stat, value in price_stats.items():\n",
    "            print(f\"  {stat}: ${value:,.2f}\")\n",
    "        \n",
    "        # Price anomalies\n",
    "        print(\"\\n‚ö†Ô∏è Price Anomaly Checks:\")\n",
    "        \n",
    "        # Check for negative prices\n",
    "        negative_prices = (df['close'] <= 0).sum()\n",
    "        print(f\"  Negative/Zero prices: {negative_prices}\")\n",
    "        \n",
    "        # Check for extreme price changes (>50% in one day)\n",
    "        df_sorted = df.sort_values('date')\n",
    "        price_changes = df_sorted['close'].pct_change().abs()\n",
    "        extreme_changes = (price_changes > 0.5).sum()\n",
    "        print(f\"  Extreme daily changes (>50%): {extreme_changes}\")\n",
    "        \n",
    "        if extreme_changes > 0:\n",
    "            extreme_dates = df_sorted[price_changes > 0.5]['date'].tolist()\n",
    "            print(f\"    Dates with extreme changes: {extreme_dates[:5]}\")\n",
    "    \n",
    "    # Duplicate checks\n",
    "    print(\"\\nüîÑ Duplicate Checks:\")\n",
    "    date_duplicates = df['date'].duplicated().sum()\n",
    "    print(f\"  Duplicate dates: {date_duplicates}\")\n",
    "    \n",
    "    # Source distribution\n",
    "    if 'source' in df.columns:\n",
    "        print(\"\\nüìà Data Source Distribution:\")\n",
    "        source_counts = df['source'].value_counts()\n",
    "        for source, count in source_counts.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {source}: {count} records ({pct:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': missing_summary.to_dict(),\n",
    "        'date_range': (df['date'].min(), df['date'].max()),\n",
    "        'price_stats': price_stats.to_dict() if 'close' in df.columns else None,\n",
    "        'anomalies': {\n",
    "            'negative_prices': negative_prices if 'close' in df.columns else 0,\n",
    "            'extreme_changes': extreme_changes if 'close' in df.columns else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run quality checks on combined data\n",
    "if 'combined_btc_data' in locals() and combined_btc_data is not None:\n",
    "    quality_report = perform_data_quality_checks(combined_btc_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No combined data available for quality checks\")\n",
    "    quality_report = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 5: Data Quality Assessment\n",
    "\n",
    "**Critical Findings:**\n",
    "- [ ] Document completeness percentage by source\n",
    "- [ ] Note any data quality issues requiring cleaning\n",
    "- [ ] Record price anomalies and potential causes\n",
    "- [ ] Assess suitability for trading strategy development\n",
    "\n",
    "**Next Steps:**\n",
    "- [ ] Address missing data through interpolation or source switching\n",
    "- [ ] Implement data cleaning pipeline for anomalies\n",
    "- [ ] Set up automated data quality monitoring\n",
    "- [ ] Prepare data for EDA in next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Summary\n",
    "\n",
    "Final data export and summary for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of all data collection efforts\n",
    "collection_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'sources_attempted': ['investing.com', 'coinmarketcap', 'yahoo_finance'],\n",
    "    'sources_successful': [],\n",
    "    'total_records': 0,\n",
    "    'date_range': None,\n",
    "    'files_created': [],\n",
    "    'quality_report': quality_report\n",
    "}\n",
    "\n",
    "# Check which sources were successful\n",
    "if 'investing_df' in locals() and investing_df is not None:\n",
    "    collection_summary['sources_successful'].append('investing.com')\n",
    "    collection_summary['files_created'].append('btc_investing_raw.csv')\n",
    "\n",
    "if 'yahoo_df' in locals() and yahoo_df is not None:\n",
    "    collection_summary['sources_successful'].append('yahoo_finance')\n",
    "    collection_summary['files_created'].append('btc_yahoo_raw.csv')\n",
    "\n",
    "if 'cmc_data' in locals() and cmc_data is not None:\n",
    "    collection_summary['sources_successful'].append('coinmarketcap')\n",
    "\n",
    "if 'combined_btc_data' in locals() and combined_btc_data is not None:\n",
    "    collection_summary['total_records'] = len(combined_btc_data)\n",
    "    collection_summary['date_range'] = [\n",
    "        combined_btc_data['date'].min().isoformat(),\n",
    "        combined_btc_data['date'].max().isoformat()\n",
    "    ]\n",
    "    collection_summary['files_created'].append('btc_combined_raw.csv')\n",
    "\n",
    "# Save collection summary\n",
    "import json\n",
    "with open('../data/collection_summary.json', 'w') as f:\n",
    "    json.dump(collection_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"üìã Data Collection Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"‚úÖ Successful sources: {len(collection_summary['sources_successful'])}/{len(collection_summary['sources_attempted'])}\")\n",
    "print(f\"üìä Total records collected: {collection_summary['total_records']}\")\n",
    "print(f\"üìÅ Files created: {len(collection_summary['files_created'])}\")\n",
    "print(f\"üèÅ Ready for EDA: {'‚úÖ' if collection_summary['total_records'] > 0 else '‚ùå'}\")\n",
    "\n",
    "if collection_summary['total_records'] > 0:\n",
    "    print(f\"\\nüìÖ Date range: {collection_summary['date_range'][0]} to {collection_summary['date_range'][1]}\")\n",
    "    print(\"\\nüéØ Next step: Run notebook 02_eda_analysis.ipynb for exploratory data analysis\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data collected - check API keys and network connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Observation: Data Collection Complete\n",
    "\n",
    "**Summary of Results:**\n",
    "- [ ] Record final success rate of all data sources\n",
    "- [ ] Document total data points collected\n",
    "- [ ] Note any persistent issues with specific sources\n",
    "- [ ] Confirm readiness for EDA phase\n",
    "\n",
    "**Key Takeaways for Trading Strategy:**\n",
    "1. Data reliability ranking by source\n",
    "2. Recommended fallback strategy for data outages\n",
    "3. Data freshness considerations for live trading\n",
    "4. Quality thresholds for strategy execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}